{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:15:42.192525Z",
     "start_time": "2025-11-11T02:15:42.178632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from task.multi_task import filter_tasks\n",
    "from utils.stim_io import HvMImageLoader, HvMMetaData, HvMImageMapper, _subpath_after"
   ],
   "id": "45ed50d1df786ebf",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:15:42.656493Z",
     "start_time": "2025-11-11T02:15:42.637627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hvm_dir = '/Users/markbai/PycharmProjects/RNN_NatAbs/data/original'\n",
    "OUTPUT_CSV = \"../resources/interdms_position_identity_trials.csv\"\n",
    "IMAGES_DIR = Path('images')\n",
    "BG_FP = IMAGES_DIR / 'gray_background.png'\n",
    "N_STIMS = 100\n",
    "N_OBJS = N_STIMS // 2\n",
    "\n",
    "SESSIONS = [1, 2, 3, 4, 5]\n",
    "TRIALS_PER_SESSION = 20\n",
    "FRAMES_PER_TRIAL = 4\n",
    "RANDOM_SEED = 2025\n",
    "GRID_SIZE = 3\n",
    "ACTION_MAP = {\n",
    "    0: 'b',  # no action\n",
    "    1: 'x',  # match\n",
    "    2: 'None',  # non-match\n",
    "}\n",
    "\n",
    "random.seed(RANDOM_SEED)"
   ],
   "id": "1969165bb93edebe",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:15:45.694159Z",
     "start_time": "2025-11-11T02:15:45.658106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pick_objs(df, n_objs):\n",
    "    catids = list()\n",
    "    cats = sorted(df['cat_1b'].unique())\n",
    "\n",
    "    obj_per_cat = n_objs // len(cats)\n",
    "    remainder = n_objs % len(cats)\n",
    "\n",
    "    # main balanced allocation\n",
    "    for cat in cats:\n",
    "        objs_in_cat = sorted(df.loc[df['cat_1b'] == cat, 'id_1b'].unique())\n",
    "        chosen = random.sample(objs_in_cat, obj_per_cat)\n",
    "        for obj in chosen:\n",
    "            catids.append((cat, obj))\n",
    "\n",
    "    # randomly choose remainder categories\n",
    "    if remainder > 0:\n",
    "        extra_cats = random.sample(cats, k=remainder)  # ðŸ‘ˆ random instead of cats[:remainder]\n",
    "        for cat in extra_cats:\n",
    "            objs_in_cat = sorted(df.loc[df['cat_1b'] == cat, 'id_1b'].unique())\n",
    "            already = {obj for (c, obj) in catids if c == cat}\n",
    "            available = list(set(objs_in_cat) - already)\n",
    "            if available:\n",
    "                catids.append((cat, random.choice(available)))\n",
    "    return catids\n",
    "\n",
    "\n",
    "def pick_locations(df, catid_to_positions, chosen_objs, loc_c=5):\n",
    "    rows = list()\n",
    "    for catid in chosen_objs:\n",
    "        cat, obj = catid\n",
    "        positions = sorted(catid_to_positions[catid])\n",
    "\n",
    "        # make sure there is a consistent location for task sampling\n",
    "        chosen_pos = [loc_c]\n",
    "        positions.remove(loc_c)\n",
    "        chosen_pos.append(random.sample(positions, k=1)[0])\n",
    "        for pos in chosen_pos:\n",
    "            subset = df[\n",
    "                (df[\"cat_1b\"] == cat) &\n",
    "                (df[\"id_1b\"] == obj) &\n",
    "                (df[\"pos_1b\"] == pos)\n",
    "                ]\n",
    "            row = subset.sample(1)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def sample_df(hvm_dir, n_objs, grid_size, df_path=None):\n",
    "    meta = HvMMetaData(hvm_dir)\n",
    "    img_loader = HvMImageLoader(\n",
    "        root_dir=hvm_dir,\n",
    "        metadata=meta,\n",
    "        preload_images=False,\n",
    "    )\n",
    "    img_loader.prepare_for_tasks(grid_size)\n",
    "    df = img_loader.df\n",
    "    if df_path is not None and os.path.isfile(df_path):\n",
    "        print(f'Loading subset csv at {df_path}')\n",
    "        df_subset = pd.read_csv(df_path)\n",
    "    else:\n",
    "        catid_to_positions = img_loader._task_cache.catid_to_positions\n",
    "        chosen_objs = pick_objs(df, n_objs)\n",
    "        rows = pick_locations(df, catid_to_positions, chosen_objs, loc_c=5)\n",
    "        df_subset = pd.concat(rows, ignore_index=True)\n",
    "        df_subset.to_csv(df_path)\n",
    "        print(f'Saving subset csv at {df_path}')\n",
    "\n",
    "    img_loader.df = df_subset\n",
    "    img_loader._task_cache = None\n",
    "    img_loader.prepare_for_tasks(grid_size)\n",
    "    return img_loader, meta"
   ],
   "id": "301ff4bbd9299976",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:15:52.130627Z",
     "start_time": "2025-11-11T02:15:47.045723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_dir = Path.cwd().parent\n",
    "images_dir = project_dir / 'resources'\n",
    "df_path = images_dir / 'subset.csv'\n",
    "\n",
    "img_loader, meta_data = sample_df(hvm_dir, N_OBJS, GRID_SIZE, df_path=df_path)\n",
    "img_loader._task_cache.catid_to_positions"
   ],
   "id": "769657bb560e0697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 0 rows from the original df\n",
      "found 5760 local images after filtering\n",
      "normalizing with stats: ([0, 0, 0], [1, 1, 1])\n",
      "Loading subset csv at /Users/markbai/PycharmProjects/abs_nat_psychopy_mturk/resources/subset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, 2): array([5, 9]),\n",
       " (1, 6): array([5, 9]),\n",
       " (1, 4): array([5, 8]),\n",
       " (1, 8): array([2, 5]),\n",
       " (1, 1): array([5, 8]),\n",
       " (1, 5): array([5, 7]),\n",
       " (2, 7): array([1, 5]),\n",
       " (2, 5): array([5, 7]),\n",
       " (2, 2): array([4, 5]),\n",
       " (2, 1): array([5, 7]),\n",
       " (2, 4): array([4, 5]),\n",
       " (2, 6): array([5, 6]),\n",
       " (3, 2): array([3, 5]),\n",
       " (3, 1): array([5, 6]),\n",
       " (3, 7): array([5, 7]),\n",
       " (3, 8): array([5, 6]),\n",
       " (3, 4): array([5, 7]),\n",
       " (3, 6): array([2, 5]),\n",
       " (4, 7): array([3, 5]),\n",
       " (4, 1): array([5, 6]),\n",
       " (4, 5): array([5, 6]),\n",
       " (4, 8): array([3, 5]),\n",
       " (4, 6): array([1, 5]),\n",
       " (4, 4): array([5, 7]),\n",
       " (5, 4): array([3, 5]),\n",
       " (5, 1): array([5, 8]),\n",
       " (5, 6): array([5, 7]),\n",
       " (5, 2): array([4, 5]),\n",
       " (5, 8): array([5, 6]),\n",
       " (5, 5): array([1, 5]),\n",
       " (6, 6): array([5, 8]),\n",
       " (6, 5): array([1, 5]),\n",
       " (6, 1): array([4, 5]),\n",
       " (6, 7): array([4, 5]),\n",
       " (6, 8): array([4, 5]),\n",
       " (6, 3): array([4, 5]),\n",
       " (7, 4): array([3, 5]),\n",
       " (7, 2): array([5, 9]),\n",
       " (7, 1): array([3, 5]),\n",
       " (7, 7): array([5, 6]),\n",
       " (7, 6): array([1, 5]),\n",
       " (7, 8): array([2, 5]),\n",
       " (8, 3): array([5, 8]),\n",
       " (8, 5): array([4, 5]),\n",
       " (8, 8): array([3, 5]),\n",
       " (8, 1): array([3, 5]),\n",
       " (8, 4): array([4, 5]),\n",
       " (8, 2): array([5, 7]),\n",
       " (8, 6): array([5, 6]),\n",
       " (7, 5): array([3, 5])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:42:20.653902Z",
     "start_time": "2025-11-10T22:42:20.448699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloaders_dict = filter_tasks([20])\n",
    "datasets = dict()\n",
    "for task_name, (DatasetClass, kwargs) in dataloaders_dict.items():\n",
    "    dataset_kwargs = kwargs.copy()\n",
    "    dataset_kwargs.update({\n",
    "        'hvm_loader': img_loader,\n",
    "        'pad_to': FRAMES_PER_TRIAL,\n",
    "        'dataset_size': len(SESSIONS) * TRIALS_PER_SESSION,\n",
    "    })\n",
    "    # Instantiate the dataset\n",
    "    tmp_task = DatasetClass(**dataset_kwargs)\n",
    "    tmp_task.reset()\n",
    "    datasets[task_name] = tmp_task\n",
    "datasets"
   ],
   "id": "27b7d9937792772c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interdms_ABAB_position_identity': <task.dms.InterDMSDataset at 0x17dbcec40>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T00:00:01.005289Z",
     "start_time": "2025-11-10T23:59:54.164088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trial_to_row(ds, emb, action, session_id):\n",
    "    zero_mask = torch.all(torch.isclose(\n",
    "        emb,\n",
    "        torch.tensor(0.0, dtype=emb.dtype)\n",
    "    ), dim=1)\n",
    "    nonzero_idx = torch.nonzero(~zero_mask).squeeze(1)\n",
    "\n",
    "    subset = emb[nonzero_idx]\n",
    "    img_mapper = HvMImageMapper(ds)\n",
    "    files, decode_tuples = img_mapper._batch_decode_and_find(subset)\n",
    "    fp_list = list()\n",
    "    for i in range(FRAMES_PER_TRIAL):\n",
    "        if i in nonzero_idx:\n",
    "            fp = files.pop(0)\n",
    "            fp = Path(fp)\n",
    "            fp = _subpath_after(\n",
    "                fp, segment='HvM_with_discfade'\n",
    "            )\n",
    "            fp = IMAGES_DIR / fp\n",
    "        else:\n",
    "            fp = BG_FP\n",
    "        fp_list.append(str(fp))\n",
    "\n",
    "    row = {\n",
    "        'session': session_id,\n",
    "    }\n",
    "    for i, (a, fp) in enumerate(zip(action, fp_list)):\n",
    "        if i != 0 and a != 2:  \n",
    "            # only save action frames\n",
    "            row[f'act{i + 1}'] = ACTION_MAP[a.item()]\n",
    "\n",
    "        row[f'stim{i + 1}'] = fp\n",
    "    return row\n",
    "\n",
    "\n",
    "def build_csv_from_dataset(dataset, out_csv: str = OUTPUT_CSV) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    session_id = 1\n",
    "    for i, (emb, action, task_index) in enumerate(dataset):\n",
    "        if i % TRIALS_PER_SESSION == 0 and i > 0:\n",
    "            session_id += 1\n",
    "        rows.append(trial_to_row(dataset, emb, action, session_id))\n",
    "    stim_cols = [f\"stim{i}\" for i in range(1, FRAMES_PER_TRIAL + 1)]\n",
    "    act_cols = [f\"act{i}\" for i in range(3, FRAMES_PER_TRIAL + 1)]\n",
    "    df = pd.DataFrame(rows, columns=[\"session\"] + stim_cols + act_cols)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved: {out_csv} (rows={len(df)})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def move_images_to_local_folder(df: pd.DataFrame, images_dir: Path = IMAGES_DIR) -> pd.DataFrame:\n",
    "    project_dir = Path.cwd().parent\n",
    "    images_dir = project_dir / 'resources' / images_dir\n",
    "    shutil.rmtree(images_dir / 'Variation00_20110203', True)\n",
    "    shutil.rmtree(images_dir / 'Variation03_20110128', True)\n",
    "    shutil.rmtree(images_dir / 'Variation06_20110131', True)\n",
    "\n",
    "    fps = df['filename'].unique()\n",
    "    for fp in fps:\n",
    "        fp = Path(fp)\n",
    "        new_fp = _subpath_after(\n",
    "            fp, segment='HvM_with_discfade'\n",
    "        )\n",
    "        new_fp = images_dir / new_fp\n",
    "        new_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(fp, new_fp.absolute())\n",
    "    return\n",
    "\n",
    "\n",
    "move_images_to_local_folder(img_loader.df, IMAGES_DIR)\n",
    "inter_dms = datasets[\"interdms_ABAB_position_identity\"]\n",
    "out_df = build_csv_from_dataset(inter_dms, out_csv=OUTPUT_CSV)"
   ],
   "id": "67620bf757def461",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../resources/interdms_position_identity_trials.csv (rows=100)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:16:33.110280Z",
     "start_time": "2025-11-11T02:16:33.062334Z"
    }
   },
   "cell_type": "code",
   "source": "img_loader.df['var'].value_counts()",
   "id": "2c390f990bf9f549",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var\n",
       "3    51\n",
       "6    40\n",
       "0     9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:47:22.135109Z",
     "start_time": "2025-11-10T22:47:22.056995Z"
    }
   },
   "cell_type": "code",
   "source": "inter_dms.actions",
   "id": "24514725ce491bdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 0, 0],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 0, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1],\n",
       "        [2, 2, 1, 0],\n",
       "        [2, 2, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:42:23.020432Z",
     "start_time": "2025-11-10T22:42:22.999705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_dir = Path.cwd().parent\n",
    "images_dir = project_dir / 'resources'\n",
    "for col in [c for c in out_df.columns if 'stim' in c]:\n",
    "    for img in out_df[col]:\n",
    "        if not os.path.isfile(images_dir / Path(img)):\n",
    "            print(img)"
   ],
   "id": "c69187c4ce77c75a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "45e635cba8e7bb7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
